{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN tf2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "45wWS4Ng3Qja"
      },
      "source": [
        "%cd /content\n",
        "! rm -rf gan-tools2\n",
        "!git clone --single-branch --depth=1 --branch main https://github.com/hannesdm/gan-tools2.git\n",
        "%cd /content/gan-tools2\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras import initializers\n",
        "from keras import Sequential\n",
        "from keras.layers import  Dense\n",
        "from core.gan import GAN, WGAN, WGAN_GP\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "plt.rcParams['axes.grid'] = False\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpGHuTd85t06"
      },
      "source": [
        "## Wasserstein GAN\n",
        "We will train a standard GAN and a Wasserstein GAN on the mnist data. <br/>\n",
        "Both variants have a relatively simple fully connected architecture to allow for fast training. This will inevitable produce worse results than larger or specialized models (cfr. DCGAN). <br/>\n",
        "The Wasserstein GAN implementation follows the paper of Arjovsky et al. <br/>\n",
        "You are encouraged to change the parameters and architecture of the model. If you do, do **not** change the **input_dim** and **final layer**. <br/>\n",
        "**Exercise** Compare the performance of the different GAN formulations over\n",
        "the different iterations, do you see an improvement in stability and quality of the generated samples? <br/>Elaborate based on\n",
        "the knowledge you have gained about optimal transport and the Wasserstein distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G6u-c2151PP"
      },
      "source": [
        "(X_train_mnist, Y_train_mnist), (_, _) = mnist.load_data()\n",
        "X_train_mnist = X_train_mnist.reshape((-1, 28*28))\n",
        "X_train_mnist = X_train_mnist.astype('float32') / 127.5 - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dxwND5A6ChH"
      },
      "source": [
        "\n",
        "def mnist_generator_model():\n",
        "  generator = Sequential()\n",
        "  generator.add(Dense(256, input_dim=100, activation='relu'))\n",
        "  generator.add(Dense(256, activation='relu'))\n",
        "  generator.add(Dense(784, activation='tanh'))\n",
        "  return generator\n",
        "\n",
        "def mnist_discriminator_model():\n",
        "  discriminator = Sequential()\n",
        "  discriminator.add(Dense(256, input_dim=784, activation='relu'))\n",
        "  discriminator.add(Dense(256, activation='relu'))\n",
        "  discriminator.add(Dense(1, activation='linear'))\n",
        "  return discriminator\n",
        "\n",
        "def mnist_wgan_discriminator_model():\n",
        "  discriminator = Sequential()\n",
        "  discriminator.add(Dense(256, input_dim=784, activation='relu'))\n",
        "  discriminator.add(Dense(256, activation='relu'))\n",
        "  discriminator.add(Dense(1, activation='linear'))\n",
        "  return discriminator\n",
        "\n",
        "def mnist_weight_clipping_discriminator_model():\n",
        "  discriminator = Sequential()\n",
        "  discriminator.add(Dense(256, input_dim=784, activation='relu'))\n",
        "  discriminator.add(Dense(256, activation='relu'))\n",
        "  discriminator.add(Dense(1, activation='linear'))\n",
        "  return discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrFpaw_49QAN"
      },
      "source": [
        "## Train the standard GAN\n",
        "The parameters **batches**, **batch_size** and **plot_interval** may be changed if wanted. <br/>\n",
        "Remember that the execution may be interrupted at any time by clicking the stop button or by selecting the 'interrupt execution' option in the runtime menu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQvA6cyW9MI6"
      },
      "source": [
        "mnist_gan = GAN(discriminator=mnist_discriminator_model(), generator=mnist_generator_model())\n",
        "mnist_gan.train(X_train_mnist, steps = 20000, batch_size=64, plot_interval = 500, image_shape=(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEm1VpMq-TH9"
      },
      "source": [
        "## Train the Wasserstein GAN with Weight Clipping\n",
        "The Discriminator in the GAN framework now performs the role of a critic, instead of a classifier.\n",
        "The original way to enforce the Lipschitz constraint on the critic is by making sure the weights lie in a compact space. This can be done by clipping the weights after each gradient update. This implementation follows the work by Arjovsky et al. See https://arxiv.org/pdf/1701.07875.pdf  <br/>\n",
        "\n",
        "The parameters **batches**, **batch_size** and **plot_interval** may be changed if wanted. <br/>\n",
        "Remember that the execution may be interrupted at any time by clicking the stop button or by selecting the 'interrupt execution' option in the runtime menu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7MfT6D_9_ZF"
      },
      "source": [
        "mnist_wgan = WGAN(discriminator=mnist_weight_clipping_discriminator_model(), generator=mnist_generator_model(), n_critic=5)\n",
        "mnist_wgan.train(X_train_mnist, steps=20000, batch_size=64,\n",
        "                                plot_interval = 500, image_shape=(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GlYGPTPaVgb"
      },
      "source": [
        "## Wasserstein GAN with Gradient Penalty\n",
        "A more natural way of enforcing the Lipschitz constraint in the Wasserstein GAN formulation is by penalizing the norm of the gradient of the critic. This implementation follows the work of Gulrajani et al. See https://arxiv.org/pdf/1704.00028.pdf <br/>\n",
        "The parameters **batches**, **batch_size** and **plot_interval** may be changed if wanted. <br/>\n",
        "Remember that the execution may be interrupted at any time by clicking the stop button or by selecting the 'interrupt execution' option in the runtime menu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BMAFmMKaO_X"
      },
      "source": [
        "mnist_wgan_gp = WGAN_GP(discriminator=mnist_wgan_discriminator_model(), generator=mnist_generator_model(),n_critic=5)\n",
        "mnist_wgan_gp.train(X_train_mnist, steps=20000, batch_size=64,\n",
        "                                plot_interval = 500, image_shape=(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vis\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Plot the final loss curves\n",
        "def moving_average(a, n=10) :\n",
        "    s = np.cumsum(a, dtype=float)\n",
        "    s[n:] = s[n:] - s[:-n]\n",
        "    return s[n - 1:] / n\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.plot(moving_average(mnist_gan.d_losses), c=\"blue\", label=\"D Loss\")\n",
        "plt.plot(moving_average(mnist_gan.g_losses), c=\"red\", label=\"G Loss\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v-OdbXbbQvxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vis\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Plot the final loss curves\n",
        "def moving_average(a, n=10) :\n",
        "    s = np.cumsum(a, dtype=float)\n",
        "    s[n:] = s[n:] - s[:-n]\n",
        "    return s[n - 1:] / n\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.plot(moving_average(mnist_wgan.d_losses), c=\"blue\", label=\"D Loss\")\n",
        "plt.plot(moving_average(mnist_wgan.g_losses), c=\"red\", label=\"G Loss\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YMyVf7Z6Q42z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vis\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Plot the final loss curves\n",
        "def moving_average(a, n=10) :\n",
        "    s = np.cumsum(a, dtype=float)\n",
        "    s[n:] = s[n:] - s[:-n]\n",
        "    return s[n - 1:] / n\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.plot(moving_average(mnist_wgan_gp.d_losses), c=\"blue\", label=\"D Loss\")\n",
        "plt.plot(moving_average(mnist_wgan_gp.g_losses), c=\"red\", label=\"G Loss\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7NWatmrYQ7Gr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}